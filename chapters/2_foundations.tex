\chapter{Foundations}
\label{chap:foundation}

\section{Genetic Algorithm}
\label{chap:foundation:genetic_algorithm}


\paragraph{General}
Pioneered by John Holland (1974) \todo{Find paper}, Genetic Algorithms (GAs) imitate natural selection and the Darwinian principle called "survival of the fittest". Genetic Algorithms search the solution space using a population of individuals. Each individual contains one chromosome, which serves as a candidate solution. Using genetic operations, the individuals mate among themselves and mutate independently. Which individual is chosen depends on a fitness function, which defines the search problem. Individuals that do poorly on the given problem die out, while "strong" individuals propagate.
Genetic Algorithms iteratively (each iteration is call "generation") optimize by manipulating a population of potential solutions (\cite{srinivas_genetic_1994}).
Chromosomes consist of individual genes, which form a solution to the search problem. Over generations using genetic operations, gene values and position will be optimized, resulting in interativly better solutions (\cite{srinivas_genetic_1994}).
This masters thesis will utilize generational genetic algorithms, where each generation, the entire population is replaces. Contrary to that, steady state genetic algorithms only replace a small fraction of the population at a time (\cite{srinivas_genetic_1994}).

Genetic algorithms are a subclass of evolutionary algorithms (\cite{mills_determining_2015}).


\paragraph{Pros}
This search method on the basis of biological principles allows trough its population for a global and dispersed search which avoids various shortcommings of local search techniques (\cite{grefenstette_optimization_1986}). Especially on difficult search spaces with lots of local optima, a genetic alogorithm is less prone to get stuck on a substandard solution (\cite{katoch_review_2021}, \cite{xia_genetic_2019}, \cite{majumdar_genetic_2015}). Genetic Algorithms offer advantages for complex optimization and non-deterministic polynomial (NP-hard) problems (\cite{hussain_trade-off_2020}).

\enquote{GAs can find good solutions within a large, ill-defined search space, and can be readily adapted to a wide variety of search problems (Mitchell, 1998)} (\cite{mills_determining_2015})

\cite{grefenstette_optimization_1986} further emphasis its ability to outperform gradient techniques on difficult problems such as "as optimizations involving discontinuous, noisy, high-dimensional, and multimodal objective functions." Adding, that base Genetic Algorithms their effectiveness on their "ability to exploit efficiently this vast amount of accumulating knowledge by means of relatively simple selection mechanisms".

\enquote{The most attractive feature of GA is that it has the ability to explore the search space with the help of the entire population of individuals.} (\cite{hussain_trade-off_2020})


Further advantages are the "high implicit parallelism", which makes genetic algorithms "numerically very efficient" (\cite{marsili_libelli_adaptive_2000}).

\enquote{The power and simplicity of GA make it popular for even largescale optimization problems. The main advantage of GA is that it does not require neither mathematical expression of response surfaces nor any derivative or gradient information} (\cite{boyabatli_parameter_2004})


\paragraph{Basic Structure}
At its core, a genetic algorithm uses a list of simple evolution inspired functions. A basic genetic algorithm can be seen here, according to \cite{srinivas_genetic_1994}. \todo{Probably ref Holland or Dejong}

\begin{lstlisting}[language=C, tabsize=4]
	simple_genetic_algorithm() 
	{
		initialize_population();
		evaluate_population();
		for(int i = 0; i < num_of_generations; i++) 
		{
			select_individuals_for_next_population();
			perform_crossover();
			perform_mutation();
			evaluate_population();
		}
	}
\end{lstlisting}


First, the population is initialized randomly and its individuals are subsequently evaluated using the fitness function. The following steps are now repeated until some stopping criteum is triggered (\cite{grefenstette_optimization_1986}) (in this case, it is just a maximum on the number of generations).
The individuals are chosen using a selection operation, which takes their fitness value under consideration. Next, under some crossover probabilty, the selected individals mate, producing new offspring. This surfs the purpose of exchanging information between the chromosomes as genes. Futher, a handfull of indivduals (selected using a mutation probabilty) get mutated, which adds small variations to the chromomsomes, thus adding new variation to the population.
Finally the new population gets evaluated.


\paragraph{Problems}
According to \cite{hussain_trade-off_2020}, genetic algorithms suffer from an exploration vs exploitation dilemma. If the algorithm converges too quickly to a solution, most of the search space might not have yet been explored, thus increasing the probability of getting stuck at only a local optimum.
In contrast to that, a low convergence rate will be time consimung and result in wasted compuational resources.

\enquote{Finding a balance between exploration and exploitation has been a difficult-to-achieve goal from the beginning.} (\cite{kacprzyk_parameter_2007})

Genetic algorithms also require an application specific encoding of genes, which requires time and domain specific knowledge. Due to know real consus on the "best parameter settings" (see section \ref{chap:hyperparameter_tuning}, their optimal selection proves to be difficult.

"
Premature convergence is a common issue for GA. It can lead to the loss of alleles that makes it difficult to identify a gene [15]. Premature convergence states that the result will be suboptimal if the optimization problem coincides too early.
"\cite{katoch_review_2021}



\subsection{Genes}
\label{chap:foundation:ga:encoding}

The encoding of the genes proved to be, besides the hyperparamter tuning, the main challenge of setting up a genetic algorithm pipeline.
A simple and commonly used representation for genes is binary encoding. As suggested by its name, a gene can take the for as 0 or 1. Futher common encodings are octal and hexadecimal representations (\cite{srinivas_genetic_1994}, \cite{katoch_review_2021}).

In case a custom encoding needs to be used, the genetic operation crossover and mutation have to be tailored accordingly. \cite{srinivas_genetic_1994} suggests, to use integer representations in case a optimization problem has real-valued continuous variables. Theses variables are linearly mapped to integers defined in a specific range. It is also possible to again represent these integers as binary encodings.

Various other encodings are available, often very problem specific. For example tree encodings allows for genes to represent programming functions, leading to a path of genetic algorithms called "genetic programming" (\cite{katoch_review_2021}).


\subsection{Control Parameter of a Genetic Algorithm}
Hyperparamter have a huge influence on the performance of a Genetic Algorihm. They have an impact on the "convergin" ...
It has been shown, that there is no universal hyperparamter set and that it needs to be optimized on a per "problem" basis.

\subsubsection{Num of generations}
The Number of Generation defines the duration of a GA. As long as the algorihtm has not converged, ....?
For my testing, using a generation size of 30 was almost always sufficient, and will thus mostly be used.

\subsubsection{Population Size}
Controlling the Population Size, which defines the number of individuals per generation, has a direct impact on the performance of the genetic algorithm.
Research seams to be in agreement, that a small population leads to less diverse individuals and might provide an insufficient sample size. This can lead to a premature convergence to a local optimum.
Increasing the population size will allow the genetic algorithm to perform a more informed search. However computation time will suffer due to the larger number of individuals per generation as well as slower convergence to an optimum (\cite{grefenstette_optimization_1986}, \cite{katoch_review_2021}, \cite{kacprzyk_parameter_2007}).
\cite{kacprzyk_parameter_2007} also suggest, that "complex, multi-peaked landscapes may require populations of 100s or even 1000s of parents in order to have some reasonable chance of finding globally optimal solutions"
This Exploration-Exploitation problem is at the core of the Genetic Algorithm.

Increasing population size will increase the degree if parallelism, as each individual represents one search point (\cite{mills_determining_2015}).


\subsubsection{Selection}
The selection operator chooses two parents for crossover and mutation opteriation until the list offsprings has reached the desired population size. 
The selection operator will favour individuals based on their fitness value, ensuring their increased representation from generation to generation (\cite{srinivas_genetic_1994}). Weak solutions thus will be discarded over time.
Different selection methods like stochastic uniform remainder, random selection, rank selection, roulette wheel selection and tournament selection exist (\cite{majumdar_genetic_2015}). The choice of selection method highly affects the performance of the Genetic Algorithm (\cite{hussain_trade-off_2020}).

The selection algorithm again needs to satisfy two requirements. On the one hand, high selection pressure will lead to decreased diversity in the population resulting in premature convergence (\cite{katoch_review_2021}). This results in the algorithm behaviong more like a local search method like a hill-climber or a “greedy” algorithm (\cite{kacprzyk_parameter_2007}).
The initial low average fitness value of the population will in combination with a few good performing individuals will lead to them overtaking the population, drastically reducing diversity.
On the other hand, a low selection pressure will struggle to converge at an optimum.

\cite{hussain_trade-off_2020} propose a selection method where initially a low selection pressure is applied, while increasing it to the end, arguing, that "trade-off between these two competing criteria, an adjustable selection pressure must desired".








"
The proportionate selection scheme allocates approximately equal numbers of offspring to all strings, thereby depleting the driving force that promotes better strings. Scaling mechanisms and rankbased selection schemes overcome these two problems.
"\cite{srinivas_genetic_1994}



ROULETTE


"
The first selection mechanism for GA was fitness proportional selection (FPS), which was introduced by Holland [1]. Now, it has become the most prevalent selection approach which used the concept of proportionality. It works as the fitness value of each individual in a population corresponds to the area of roulette wheel proportions. Then, an individual is marked by the roulette wheel pointer after it has spun. This operator gives individuals, a probability pi of being selected Eq. (1) that is directly proportionate to their fitness:
However, the difficulty is encountered when a significant difference appears in the fitness values [14,17,18]. The scaling problem which is the major drawback of this scheme was first pointed out by Grefenstette [19]. It has happened when population evolves, the ratio between the variance and the fitness average becomes increasingly small. The selection pressure, therefore, drops as the population converges [7]. On the other hand, high selection pressure may lead to premature convergence to a sub-optimal solution.
"\cite{hussain_trade-off_2020}
"
Roulette wheel selection maps all the possible strings onto a wheel with a portion of the wheel allocated to them according to their fitness value. This wheel is then rotated randomly to select specific solutions that will participate in formation of the next generation [88]. However, it suffers from many problems such as errors introduced by its stochastic nature.
"\cite{katoch_review_2021}
"
De Jong and Brindle modified the roulette wheel selection method to remove errors by introducing the concept of determinism in selection procedure. Rank selection is the modified form of Roulette wheel selection. It utilizes the ranks instead of fitness value. Ranks are given to them according to their fitness value so that each individual gets a chance of getting selected according to their ranks. Rank selection method reduces the chances of prematurely converging the solution to a local minima [88].
"\cite{katoch_review_2021}


TOURNAMENT
"
The tournament selection (TS) is also widely used as an alternative to FPS. In TS, first, randomly select the t (where t is the predefined tournament size) individuals from the population and then they compete against each other based on their fitness. An individual with higher fitness value is declared as a winner and selected for mating process. The selection pressure can be adjusted with change the tournament size [7]. Usually, the most used tournament size is 2 (binary tournament selection (BTS)), which is the simplest form of TS [21]. However, the larger tournament size can be used to enhance the competition among individuals, but it leads to loss of population diversity [22,23].
"\cite{hussain_trade-off_2020}

"
Tournament selection technique was first proposed by Brindle in 1983. The individuals are selected according to their fitness values from a stochastic roulette wheel in pairs. After selection, the individuals with higher fitness value are added to the pool of next generation [88].
"\cite{katoch_review_2021}

"Qualitative analysis of the selection strategies is depicted, and the numerical experiments show that SGA with tournament selection strategy converges much faster than roulette wheel selection."
\cite{jinghui_zhong_comparison_2005}
"

Roulette wheel selection is the most frequently used selection strategy.
"\cite{jinghui_zhong_comparison_2005}
"
Tournament selection is also a selection strategy which selects individuals based on their fitness value. The basic idea of this strategy is to select the individual with the highest fitness value from a certain number of individuals in the population into the next generation. In the tournament selection, there is no arithmetical computation based on the fitness value, but only comparison between individuals by fitness value. The number of the individuals taking part in the tournament is called tournament size.
"\cite{jinghui_zhong_comparison_2005}

"
1) Randomly select several individuals from the population to take part in the tournament. Choose the individual that has the highest fitness value from the individuals selected above by comparing the fitness value of each individual. Then the chosen one is copied into the next generation of the population. 2) Repeat step1 n times where n is the number of individuals of the population.
"\cite{jinghui_zhong_comparison_2005}
"
From the results shown above, SGA using tournament selection always obtains the satisfied solutions with more times at earlier generations than roulette wheel selection. This indicates SGA based on tournament selection converges more quickly than roulette wheel selection.
"\cite{jinghui_zhong_comparison_2005}

OTHER
"
The most popular technique is the linear rank selection (LRS) scheme proposed by Baker [20]. It sorts the individuals in the sequence as worst to best according to the fitness and allocates them a survival probability proportional to their rank order. After this task, a sampling procedure (i.e., roulette wheel sampling) is used to select the individuals for mating process. In this way, the LRS can maintain a constant selection pressure throughout in the sampling process, because it introduces a uniform scaling across the population.
The weakness of this scheme is that it can lead to slower convergence, because there is no significant difference between the best and other individuals. The selection probability of two consecutive chromosomes by the same amount is regardless of whether the gap between their fitness is larger or smaller [7].
"\cite{hussain_trade-off_2020}
"


Another rank-based selection scheme is exponential ranking selection (ERS). It works similar as to LRS, except for the non-linear assignment of probabilities to the individuals.
"\cite{hussain_trade-off_2020}


"
The main contribution of this article is in the development of the proposed selection approach which reduces the weakness associated with FPS and LRS in the GA procedure. The proposed approach is based on the ranking scheme which splits the individuals after ranking and then assign them probabilities for selection. This will increase the competition among individuals to be selected for mating process to regulate the selection pressure.
"\cite{hussain_trade-off_2020}

"
The LRS introduces slow convergence speed and sometimes converges to a sub-optimal solution as less fit individuals may be preserved from one generation to another. In GA, the FPS has the essence of exploitation, while LRS is influenced by exploration. The information about the relative evaluation of individuals is ignored, all cases are treated uniformly regardless of the magnitude of the problem and, finally, the schema theorem is violated. LRS prevents too quick convergence and differs from FPS in terms of selection pressure. This discussion suggests that, whenever a selection procedure is used, some kind of adaptation of the selection pressure is highly desirable.
"\cite{hussain_trade-off_2020}

"
In this research, we propose an alternative selection scheme [split rank selection (SRS)] that maintains a fine balance between exploration and exploitation.

In the proposed procedure, the individuals are ranked according to their fitness scores from worst to best, thus overcoming the fitness scaling issue. After this, split the whole population into two portions and assigning them probabilities for selection based on their ranks.
"\cite{hussain_trade-off_2020}



"
Stochastic universal sampling (SUS) is an extension to the existing roulette wheel selection method. It uses a random starting point in the list of individuals from a generation and selects the new individual at evenly spaced intervals [3]. It gives equal chance to all the individuals in getting selected for participating in crossover for the next generation. Although in case of Travelling Salesman Problem, SUS performs well but as the problem size increases, the traditional Roulette wheel selection performs relatively well [180].
"\cite{katoch_review_2021}

"
Boltzmann selection is based on entropy and sampling methods, which are used in Monte Carlo Simulation. It helps in solving the problem of premature convergence [118].
"\cite{katoch_review_2021}


"
An alternate way to avoid the twin problems that plague proportional selection is rank-based selection, which uses a fitness value-based rank of strings to allocate offspring. The scaled fitness values typicallyvary linearly with the rank of the string. The absolute fitness value of the stringdoes not directlycontrol the number of its offspring. To associate each string with a uniquerank,this approach sortsthe stringsaccordingto their fitnessvalues,introducing the drawback of additional overhead in the GA computation. Another mechanism is tournament selection. For selection,a stringmust win a competition with a randomly selected set of strings. In a k-arytournament, the best of k stringsis selectedfor the next generation.
"\cite{srinivas_genetic_1994}


ELITE
"
However, there is a possibility of information loss. It can be managed through elitism [175]. Elitism selection was proposed by K. D. Jong (1975) for improving the performance of Roulette wheel selection. It ensures the elitist individual in a generation is always propagated to the next generation. If the individual having the highest fitness value is not present in the next generation after normal selection procedure, then the elitist one is also included in the next generation automatically [88].
"\cite{katoch_review_2021}

\subsubsection{Crossover}
The crossover operation serves as the mating process between two individuals (chosen from the population by the selection operation). Its resulting offspring has a combination of both parents genes. \cite{grefenstette_optimization_1986} higlights, that this "First, it provides new points for further testing within the hyperplanes already represented in the population." and "Second, crossover intro- duces representatives of new hyperplanes into the population". 

Different types of crossovers can be chosen, the most simple approach being the single-point crossover. Here a randomly chosen point aloong the length of the chromosome is chosen. The two parents will swap their genetic information at this point, generating two new offsprings (\cite{katoch_review_2021}). Extenations are the two- or multipoint crossover operations, where chromosomes are divided by k segments, which will get exchanged. They eliminate a disadvantage of "the single-point crossover bias toward bits at the ends of strings." (\cite{srinivas_genetic_1994})

A second crossover opteration is the uniform crossover, here, based on a probability, it randomly decides to swap genes between the parents, independent of the exchange of other genes (\cite{katoch_review_2021}). 

enquote{To classify techniques, we can use the notions of positional and distributional biases. A crossover operator has positional bias if the probability that a bit is swapped depends on its position in the string. Distributional bias is related to the number of bits exchanged by the crossover operator. If the distribution of the number is nonuniform,the crossover operator has a distributional bias. Among the various crossover operators, single-point crossover exhibitsthe maximum positional bias and the least distributional bias. Uniform crossover, at the other end of the spectrum, has maximal distributional bias and minimal positional bias.} (\cite{srinivas_genetic_1994})

According to \cite{srinivas_genetic_1994} the ignoring of the genes position results in a higher disruptiveness, which has potential drawbacks. However it will be more exporatory in homogeneous populations where k-point crossovers might struggle to explore. \cite{srinivas_genetic_1994} also suggests that uniform crossover is more usefull in small populations. Larger populations inherently are more diverse, makeing k-point crossover more suitable.

Other crossover operations are analyzed in paper \todo{Find good research paper, maybe \cite{katoch_review_2021}}. 

Not only the crossover type has to be chosen, a crossover probility (or crossover rate) also needs definition. This value defines, how likely a single crossover operation is to be applied on the parent population. If no crossover will be applied, the parents will directly be used as children.
The problem of choosing a crossover probability again comes back to the exploration/exploatation balance. Higher crossover rates will introduce new structures quickly into the population, however good structures might get disrupted. A low crossover rate will result in a low exploration, resulting in stagnation (\cite{grefenstette_optimization_1986}). 

\enquote{If reproductive variation is too strong, the result is undirected random search.} (\cite{kacprzyk_parameter_2007})

\subsubsection{Mutation}
Mutation is responsible for introducing new information into the gene pool.

"
Setting the mutation probability too high may transform the search procedure into random testing, but it also helps to introduce new gene material, which promotes the exploration of new input space regions.
"\cite{klampfl_using_nodate}



"
Mutation is a secondary search operator which increases the variability of the population.
"\cite{grefenstette_optimization_1986}

"
A low level of mutation serves to prevent any given bit position from remaining forever converged to a single value in- the entire population. A high level of mutation yields an essentially random search.
"\cite{grefenstette_optimization_1986}

"
With a larger population and higher mutation rate, the population will tend to contain more variety, thus increasing the random aspects of the GA.
"\cite{grefenstette_optimization_1986}

"
The absence of mutation is also associated with poorer performance, which suggests that mutation performs an important service in refreshing lost values.
"\cite{grefenstette_optimization_1986}

"
After crossover. strings are subjected to mutation. Mutation of a bit involves flipping it: changing a 0 to 1 or vice versa. Just asp, controls the probability of a crossover, another parameter. P,,~(the mutation rate), gives the probability that a bit will be flipped.The bits of a string are independently mutated that is, the mutation of a bit does not affect the probability of mutation of other bits. The SGA treats mutation only as a secondary operator with the role of restoring lost genetic material.
"\cite{srinivas_genetic_1994}

"
Increasing the mutation probability tends to transform the geneticsearch into a random search, but it also helps reintroduce lost genetic material.
"\cite{srinivas_genetic_1994}

"
Mutation is not a conservative operator and can generate radically new buildingblocks.
"\cite{srinivas_genetic_1994}


"
Mutation: After crossover, the springs are subjected to mutation. Mutation functions make small random changes in the individuals in the population, which provide genetic diversity and enable the genetic algorithm to search a broader space. The different forms of mutation are constraint dependent, uniform, adaptive feasible etc. Mutation of a bit involves flipping it, changing between 0 to 1 and vice versa with a small mutation probability.
"\cite{majumdar_genetic_2015}

"
Mutation is an operator that maintains the genetic diversity from one population to the next population.
"\cite{katoch_review_2021}

"
If the mutation is not considered during evolution, then there will be no new information available for evolution.
"\cite{katoch_review_2021}

"In Genetic Algorithms mutation probability is usually assigned a constant value, therefore all chromosome have the same likelihood of mutation irrespective of their ®tness."\cite{marsili_libelli_adaptive_2000}

"
Mutation introduces random binary changes in a chromosome. Usually the mutation likelihood is kept constant at a low value for all bits. In this paper, a new mechanism of mutation will be introduced and the consequences of this modi®cation assessed.
"\cite{marsili_libelli_adaptive_2000}
\paragraph{Adaptive Mutation}
"It is shown in this paper that making mutation a function of ®tness produces a more ef®cient search. This function is such that the least signi®cant bits are more likely to be mutated in high-®tness chromosomes, thus improving their accuracy, whereas low-®tness chromosomes have an increased probability of mutation, enhancing their role in the search. In this way, the chance of disrupting a high-®tness chromosome is decreased and the exploratory role of low-®tness chromosomes is best exploited.
"
\cite{marsili_libelli_adaptive_2000}

"
The weak point of "classical" GAs is the total randomness of mutation, which is applied equally to all chromosomes, irrespective of their fitness. Thus a very good chromosome is equally likely to be disrupted by mutation as a bad one. On the other hand, bad chromosomes are less likely to produce good ones through crossover, because of their lack of building blocks, until they remain unchanged. They would benefit the most from mutation and could be used to spread throughout the parameter space to increase the search thoroughness. So there are two conflicting needs in determining the best probability of mutation. Usually, a reasonable compromise in the case of a constant mutation is to keep the probability low to avoid disruption of good chromosomes, but this would prevent a high mutation rate of low-fitness chromosomes. Thus a constant probability of mutation would probably miss both goals and result in a slow improvement of the population.
"\cite{marsili_libelli_adaptive_2000}


"
This paper has presented an improved search algorithm to reduce the chance that high-®tness chromosomes are muted during the search, thus losing their favourable schemata. In GAs, mutation is usually assigned a constant probability and thus all chromosome have the same likelihood of mutation irrespective of their ®tness. Conversely, making mutation a function of ®tness produces a more ef®cient search.
"\cite{marsili_libelli_adaptive_2000}

"
signi®cant bits are likely to be muted in high-®tness chromosomes, thus improving their accuracy, whereas low-®tness chromosomes are much more likely to change, enhancing their exploratory role in the search.
"\cite{marsili_libelli_adaptive_2000}

"
In all cases the new algorithm showed a faster improvement of the maximum ®tness, which was always greater than the one produced by the classical GA at the same generation.
"\cite{marsili_libelli_adaptive_2000}


"
In 2007, DeJong took a broader view, considering what was known with respect to the wider community of evolutionary algorithms (EAs). He observed that while it appears that adapting mutation rate on-line during execution provides advantages, most EAs are deployed with a default set of static parameter values that have been found quite robust in practice.
"\cite{mills_determining_2015}

"
Parameter settings optimal in the earlier stages of the search typically become inefficient during the later stages. Similarly, encodings become too coarse as the search progresses, and the fraction of the search space that the GA focuses its search on becomes progressivelysmaller. To overcome these drawbacks, several dynamic and adaptive strategiesfor varying the control parameters and encodings have been proposed. One strategy exponentially decreases mutation rates with increasing numbers of generations, to gradually decrease the search rate and disruption of strings as the population converges in the search space. Another approach considers dynamically modifying the rates at whichthe various genetic operators are used, based on their performance. Each operator is evaluated for the fitness values of strings it generates in subsequent generations. Very often, after a large fraction of the population has converged (the strings have become homogeneous), crossover becomes ineffective in searching for better strings. Typically, low mutation rates (0.001 to 0.01) are inadequate for continuing exploration. In such a situation, a dynamic approach for varying mutation rates based on the Hamming distance between strings to be crossed can be useful. The mutation rate increasesas the Hamming distancebetween strings decreases. As the strings to be crossed resemble each other to a greater extent,the capacity of crossover to generate new strings decreases, but the increased mutation rate sustainsthe search.
"\cite{srinivas_genetic_1994}


"
The purist might argue that inventing feedback control procedures for EAs is a good example of over-engineering an already sophisticated adaptive system.

A better strategy is to take our inspiration from nature and design our EAs to be self-regulating. For example, individuals in the population might contain “regulatory genes” that control mutation and recombination mechanisms, and these regulatory genes would be subject to the same evolutionary processes as the rest of the genome.
"\cite{kacprzyk_parameter_2007}

"
Perhaps the most interesting thing to note today, after more than 30 years of experimenting with dynamic parameter setting strategies, is that, with one exception, none of them are used routinely in every day practice. The one exception are the strategies used by the ES community for mutation step size adaptation.
"\cite{kacprzyk_parameter_2007}

"
My own view is that there is not much to be gained in dynamically adapt- ing EA parameter settings when solving static optimization problems. The real payoff for dynamic parameter setting strategies is when the fitness land- scapes are themselves dynamic (see, for example, [4], [5], or [32]).
"\cite{kacprzyk_parameter_2007}

"
For instance, in a GA, one may want to decrease the probability of crossover and mutation over time to avoid too high reproductive variation hindering convergence to local optima. However, in that case, one would need to set new parameters that define how much and how often each probability is decreased. Research has not yet concluded if one technique has a clear advantage over the other.
"\cite{klampfl_using_nodate}

\subsubsection{Other}
\paragraph{Diversity}
"
In initial stage of GA, the similarity between individuals is very low. The value of R should be low to ensure that the new population will not destroy the excellent genetic schema of individuals. At the end of evolution, the similarity between individuals is very high as well as the value of R should be high.
"\cite{katoch_review_2021}


\paragraph{Fitness Function}
"
The Fitness Function: The fitness function in Genetic Algorithm represents the objective function and the fitness value corresponds the performance of an individual chromosome.
"\cite{majumdar_genetic_2015}


"
Multiobjective GA (MOGA) is the modified version of simple GA. MOGA differ from GA in terms of fitness function assignment. The remaining steps are similar to GA. The main motive of multiobjective GA is to generate the optimal Pareto Front in the objective space in such a way that no further enhancement in any fitness function without disturbing the other fitness functions [123].
"\cite{katoch_review_2021}


"
The concept of Pareto dominance was introduced in multiobjective GAs. Fonseca and Fleming [56] developed first multiobjective GA (MOGA). The niche and decision maker concepts were proposed to tackle the multimodal problems. However, MOGA suffers from parameter tuning problem and degree of selection pressure.
"\cite{katoch_review_2021}

"
The majority of applications of optimization tools to subsurface remediation problems have been based on single objective optimization methods. Single objective methods can accommodate multiobjective problems in several ways, such as minimizing a weighted, linear combination of the objective functions or minimizing a single objective while transforming the remaining ob- jectives into constraints. However, these methods rely on a priori knowledge of the appropriate weights or constraint values. Furhtermore they are only capable of findig individual points on the tradeoff curve( or sureace) for each problem solution.




As already mentioned, previous approaches for solving the multiobjective problem have involved re- ducing the problem dimension, either by combining all objectives into a single objective (e.g. [27]) or optimizing one while the rest are constrained (e.g., [3]).
"\cite{erickson_multi-objective_2002}

"
True multiobjective methods have the potential to simultaneously generate all possible optimal combina- tions of objectives, with less effort than other ap- proaches. Multiobjective problems involve several objective functions, each of which is a function of de- cision ðdÞ and state variables ðsÞ. tiveproblemcanbestatedas:
"\cite{erickson_multi-objective_2002}

"
Multiobjective approaches in this category operate on the concept of ‘‘Pareto domination’’, which states that one candidate dominates another only if it is at least equal in all objectives and superior in at least one
"\cite{erickson_multi-objective_2002}


More on multiobjecte approaches can be found in \cite{erickson_multi-objective_2002}.

\paragraph{Stopping Criteria}

"
Stopping criteria: Stopping criteria determines what causes the algorithm to terminate-generations, time limit, fitness limit etc.
"\cite{majumdar_genetic_2015}

This is not used for this GA


\section{Behavior Tree}
A behavior tree is a decision tree. \todo{insert a good introduction to BT}

\subsection{Usage for GA}
Due to the fact, \todo{insert ref to discussion}, that there is no full stack available for the EGO vehicle, a solution had to be found.
In order to have the Genetic Algorithm controll only NPCs and not the EGO vehicle itselve, a behaviour tree is used.
The behaviour tree is used to controll the EGO vehicle over the action interface provided by the Traffic Manager. This is the same as the Genetic Algorithm is doing.

The behaviour tree will define which direction the EGO should take at junctions and it will realistically dodge obstacles intoduced by the Genetic Algorithm. The main goal of the BT is to make the EGO vehicle behave in a realistic way.

In a further chapter it will be dicussed if a GA with controll of the EGO (i.e. no BT will be used) lead to better cost.

While the aim of the GA is to find the most optimal solution, considering the vastness of the hyperspace, this is unlikely. Rather, we want to find the "best" local minimas. Considering the contex of Automotive testing, it is not so much of importance to find "the best fail of the ADAS/AD System", rather its important to find "all" fails.






