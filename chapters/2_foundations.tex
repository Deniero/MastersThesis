\chapter{Foundations}
\label{chap:foundations}

\section{Genetic Algorithm}
\label{sect:foundations:genetic_algorithm}
Pioneered by \cite{holland_adaptation_1992}, genetic algorithms imitate the process of natural selection and the darwinian principle called "survival of the fittest". Genetic algorithms are a subclass of evolutionary algorithms (EAs) and explore the solution space using a population of individuals (\cite{mills_determining_2015}). Each individual contains one chromosome, which serves as a candidate solution. Using genetic operations, the individuals mate among themselves and mutate independently. 

The selection of an individual is determined by a fitness function, which defines the search problem. Each individual has a fitness value corresponding to the performance of its chromosome (\cite{majumdar_genetic_2015}). Individuals that do poorly on the given problem die out, while individuals deemed "strong" propagate. Genetic algorithms optimize iteratively, with each iteration referred to as a generation. Chromosomes consist of individual genes, which form a solution to the search problem. Through successive generations using genetic operations, gene values and position will be optimized, resulting in progressively improved solutions (\cite{srinivas_genetic_1994}). Generational genetic algorithms, where the entire population is replaced each generation, will be utilized in this master's thesis. In contrast, steady state genetic algorithms only replace a small fraction of the population at a time (\cite{srinivas_genetic_1994}).

This search method on the basis of biological principles allows for a global and dispersed search through its population, which avoids various shortcomings of local search techniques (\cite{grefenstette_optimization_1986}). Especially on challenging search spaces with multiple local optima, a GA is less prone to get stuck on a substandard solution (\cite{katoch_review_2021}, \cite{xia_genetic_2019}, \cite{majumdar_genetic_2015}). Genetic algorithms offer advantages for complex optimization and non-deterministic polynomial (NP-hard) problems (\cite{hussain_trade-off_2020}).

\begin{quote}
	\begin{em}
		\enquote{GAs can find good solutions within a large, ill-defined search space, and can be readily adapted to a wide variety of search problems} (\cite{mills_determining_2015})
	\end{em}
\end{quote}

\cite{grefenstette_optimization_1986} further emphasis its ability to outperform gradient techniques on difficult problems with high-dimensional, noisy or discontinuous fitness functions by efficiently exploiting a "relatively simple selection mechanism." The main advantage of a GA in tackling these problems comes from its capability to explore the search space using its entire population (\cite{hussain_trade-off_2020}). The \enquote{high implicit parallelism} makes genetic algorithms \enquote{numerically very efficient} (\cite{marsili_libelli_adaptive_2000}). A basic GA proposed by \cite{holland_adaptation_1992} can be defined as follows:

\begin{lstlisting}[language=C, tabsize=4]
simple_genetic_algorithm() 
{
	initialize_population();
	evaluate_population();
	for(int i = 0; i < num_of_generations; i++) 
	{
		select_individuals_for_next_population();
		perform_crossover();
		perform_mutation();
		evaluate_population();
	}
}
\end{lstlisting}

The population is initialized randomly, and its individuals are subsequently evaluated using the fitness function. The following steps are iteratively repeated until some stopping criterium is triggered. The individuals are chosen using a selection operation, which takes their fitness value under consideration. A crossover rate subsequently determines which individuals mate using a crossover function. This operation serves the purpose of exchanging information between the chromosomes. In order to add variation and diversity, the resulting crossover offspring undergoes small changes using a mutation function. A mutation rate decides on which individuals this operation is applied. The newly generated population will then get evaluated for the next iteration. Different stopping methods like time limit, fitness limit, minimum convergence rate or maximum number of generations are available (\cite{majumdar_genetic_2015}).

As discussed by \cite{hussain_trade-off_2020}, genetic algorithms suffer from an exploration vs exploitation dilemma. If the algorithm converges too quickly to a solution, most of the search space might not have yet been explored, thus increasing the probability of getting stuck in a local optimum.
In contrast to that, a low convergence rate might be time consuming and result in inefficient utilization of computational resources.

\begin{quote}
	\begin{em}
		\enquote{Finding a balance between exploration and exploitation has been a difficult-to-achieve goal from the beginning.} (\cite{kacprzyk_parameter_2007})
	\end{em}
\end{quote}

Achieving this balance requires a suitable choice on both genetic functions as well as on their applied probabilities. Due to no real consensus on the best control parameter settings, their optimal selection proves to be difficult (\cite{kacprzyk_parameter_2007}).

\subsection{Chromosomes and Genes}
\label{sect:foundations:chromosomes_and_genes}
Both chromosomes as well as genes require adequate encodings in order to fit the search task. Each chromosome, existing of a list of genes, needs to represent a complete solution of the specific problem. Finding suitable encodings probed to be, besides the hyperparameter tuning, the main challenge of setting up a genetic algorithm pipeline.

While the chromosome encoding is very problem specific and heavily depends on the choice of gene encoding, there are various gene encodings suggested by the literature. A simple and commonly used representation for genes is binary encoding, where a gene can take on the form of 0 or 1. Further common encodings are octal and hexadecimal representations (\cite{srinivas_genetic_1994}, \cite{katoch_review_2021}). \cite{srinivas_genetic_1994} further suggests integer representations in case a optimization problem has real-valued continuous variables, where the objects are linearly mapped to integers defined in a specific range. It is possible to again represent these integers as binary encodings. Various other encodings are available, often very problem specific. For example tree encodings allows for genes to represent programming functions, leading to a subcategory of genetic algorithms called "genetic programming" (\cite{katoch_review_2021}). In case no gene encodings fits the given problem, a custom encoding can be used. Here the genetic operation crossover and mutation might have to be tailored accordingly. 

\subsection{Population Size}
Setting the population size, which defines the number of individuals per generation, has a direct impact on the performance of a genetic algorithm. Increasing the population size will increase the degree of parallelism in the genetic algorithm, as each individual represents one search point (\cite{mills_determining_2015}).

Research seams to be in agreement, that a small population leads to less diverse individuals and might provide an insufficient sample size, which can lead to a premature convergence to a local optimum.
A large population size will allow the GA to perform a more informed search. However computation time will suffer due to the larger number of individuals per generation and might lead to slower convergence to an optimum (\cite{grefenstette_optimization_1986}, \cite{katoch_review_2021}, \cite{kacprzyk_parameter_2007}).

\subsection{Selection}
The selection operator chooses two parents for crossover and mutation operations until the list of offsprings has reached the desired population size. Individuals are chosen based on their fitness value, ensuring their increased representation from generation to generation. Weak solutions will be discarded over time (\cite{srinivas_genetic_1994}). The selection algorithm needs to satisfy two requirements. On the one hand, high selection pressure will lead to decreased diversity in the population resulting in premature convergence (\cite{katoch_review_2021}). The algorithm will subsequently behave more like a local search method, a hill-climber or a greedy algorithm (\cite{kacprzyk_parameter_2007}). The initial low average fitness value of the population will in combination with a few good performing individuals lead to them overtaking the population, drastically reducing diversity. On the other hand, a low selection pressure will struggle to converge to an optimum. Different selection methods like Stochastic Uniform Remainder, Random Selection, Rank Selection, Roulette Wheel Selection and Tournament Selection exist (\cite{majumdar_genetic_2015}). According to \cite{hussain_trade-off_2020}, who provide a extensive comparison of different selection techniques, the choice of selection methods highly affects the performance of the genetic algorithm.

Roulette wheel selection is a popular mechanism, where each individual corresponds to an area on a roulette wheel, based on its fitness value (\cite{holland_adaptation_1992}). \cite{grefenstette_optimization_1986} points out the scaling problem as its major drawback. As the algorithm progresses, its fitness variance to mean ratio becomes increasingly small, leading to low selection pressure.
This problem can be mitigated by using ranks instead of the fitness value (\cite{katoch_review_2021}). Individuals will be sorted based on their performance, their relative rank is inserted into the roulette wheel instead. 

Tournament selection is a popular alternative to roulette wheel selection. First, a tournament size t has to be chosen. Until the desired number of offsprings is achieved, t individuals will be chosen at random from the population. Only from this smaller list, the best individual is selected. A popular tournament size is 2, larger sizes might enhance competition among individuals, however they can also has an impact on the diversity of the population (\cite{hussain_trade-off_2020}). In a comparison of different selection methods by \cite{jinghui_zhong_comparison_2005}, tournament selection was deemed to be the most performant, converging more quickly then for example roulette wheel selection.

Elite selection is an additional method which can enhance a selection operation and was proposed by \cite{de_jong_analysis_1975}. The best n individuals are automatically chosen for the next generation. Elite selection can help reduce variance, as the highest performing individuals can not be removed by randomness.

\subsection{Crossover}
The crossover operation serves as the mating process between two individuals (chosen from the population by the selection operation). Its resulting offspring is a combination of both parent genes. \cite{grefenstette_optimization_1986} highlights, that first, \enquote{it provides new points for further testing within the hyperplanes already represented in the population} and second, that \enquote{crossover introduces representatives of new hyperplanes into the population}. 

Different types of crossovers can be implemented, the most simple approach being the single-point crossover. Here a point is chosen at random along the length of the chromosome. The two parents will swap their genetic information at this position, generating two new offsprings (\cite{katoch_review_2021}). Extension are the two- or multipoint crossover operations, where chromosomes are divided by k segments, which will get exchanged. They eliminate a disadvantage of \enquote{the single-point crossover bias toward bits at the ends of strings} (\cite{srinivas_genetic_1994}). A second crossover operation is the uniform crossover where gene swaps between parents are randomly decided by a given probability, independent to the exchange of other genes (\cite{katoch_review_2021}). Various other crossover operations are shown in paper \cite{university_malaysia_of_computer_science_and_engineering_crossover_2017}.

\cite{srinivas_genetic_1994} proposed the notion of positional and distributional bias. If a crossover operation has a positional bias, the probability of a bit to be swapped depends mainly on its position. Examples are the mentioned single and multipoint crossover operators. The uniform crossover can be found at the other end of the spectrum, as it has a maximal distributional bias, completely disregarding any positional information. Ignoring the gene positions results in a higher disruptiveness, which has potential drawbacks. However it will be more exploratory in homogeneous populations where k-point crossovers might struggle. \cite{srinivas_genetic_1994} suggest that uniform crossover is more useful in small populations. Larger populations inherently are more diverse, making k-point crossover more suitable.

Not only the crossover type has to be chosen, a crossover rate also needs a declaration. This value defines, how likely a single crossover operation is to be applied on the selected individuals.
Choosing a suitable crossover probability is again influenced by the exploration-exploitation balance. Higher crossover rates will introduce new structures quickly into the population, however good sequences of genes might get disrupted. A low crossover rate will result in a low exploration which leads to stagnation (\cite{grefenstette_optimization_1986}).

\subsection{Mutation}
The mutation operation is applied after crossover as a means to maintain the diversity of the gene pool. Through small random changes, the variability of the population increases. Each individual can be exposed to mutation, irrespective of their fitness value.
A mutation rate chooses the individuals to be mutated. An additional individual mutation rate selects which specific genes of the chosen chromosome are mutated (\cite{srinivas_genetic_1994}). Depending on the gene encoding, the mutation operation might vary. In case of binary gene encoding, mutation will only flip certain bits. For custom encodings, the mutation operation needs to be tailored accordingly.

\begin{quote}
	\begin{em}
		\enquote{If the mutation is not considered during evolution, then there will be no new information available for evolution.}(\cite{katoch_review_2021})
	\end{em}
\end{quote}

The settings for mutation rates again needs to be chosen carefully. If they are set too high, the genetic algorithm might transform into random testing. In case they are set too low, the population will not be able to maintain diversity as no new genetic material is reintroduced. (\cite{klampfl_using_nodate}, \cite{grefenstette_optimization_1986}).

\subsection{Adaptive Control Parameters}
Various literature suggests adaptively controlling selection methods, crossover rates and mutation rates over the duration of a genetic algorithm (\cite{marsili_libelli_adaptive_2000}, \todo{insert papers}). The main goal is to mitigate the previously mentioned exploration-exploitation problem. In the beginning, exploration is desired, thus low selection pressure as well as high crossover and mutation rates are required. However to the end of the genetic algorithms duration, convergence to an optimum is preferred, meaning more "elite" selection methods as well as less variation due to crossover and mutation (\cite{srinivas_genetic_1994}).

\cite{marsili_libelli_adaptive_2000} proposed a method, where different mutation rates are used depending on the fitness of the individual. They claim, that a higher mutation probability for low fitness population results in a more efficient search. \cite{hussain_trade-off_2020} argue for a selection operation, where initially a low selection pressure is applied while increasing it to the end, stating that this approach will help mitigate both mentioned competing criteria, namely premature convergence as well as slow convergence.

Contrary, \cite{kacprzyk_parameter_2007} is more critical on adaptive control parameters. Although adaptive control parameter might improve the performance of a genetic algorithm for specific problems, genetic algorithms are already quite robust in practice. Adding more tunable parameters will not make the task of the researcher easier.

\begin{quote}
	\begin{em}
		\enquote{The purist might argue that inventing feedback control procedures for EAs is a good example of over-engineering an already sophisticated adaptive system.} (\cite{kacprzyk_parameter_2007})
	\end{em}
\end{quote}

\cite{kacprzyk_parameter_2007} adds, that \enquote{after more than 30 years of experimenting with dynamic parameter setting strategies, is that, with one exception, none of them are used routinely in every day practice. The one exception are the strategies used by the ES community for mutation step size adaptation.} In order to keep the number of tunable parameters low, the approach implemented by this master's thesis will not utilized adaptive control parameters.

\section{Behaviour Tree}
Behavior Trees were invented and developed by the computer game industry as a control structure of non-player characters (\cite{collendanchise_behavior_2019}). Since then, their scope of application has expanded into various application fields, including robotics, smart homes, power grids, autonomous vehicles (\cite{iovino_survey_2022}). \cite{sprague_improving_2018} show, that they can also be used as a robust control architecture for autonomous underwater vehicles. A behavior tree is a directed tree, with root, child, parent and leaf nodes. Non-leaf nodes are control flow nodes, while leaf nodes are called execution nodes. The execution of a BT is done by ticks, that propagate down starting from the root node with a given frequency to its children. All nodes only execute when recieving a tick and can return either "Running", "Success" or "Failure" (\cite{collendanchise_behavior_2019}).

Their high modularity and reactiveness are often stated as their main advantages. The high modularity stems from the fact, that different parts in the BT can be changed and modified without influencing the rest of the tree. This goes in hand with the reactiveness shown by the Behavior Tree. Per tick, safety test can be performed before going into the execution logic of the tree, making the system react immediately to a non safe situation. Adding complex details to this execution sub-tree will not jeopardise with the safety tests (\cite{sprague_improving_2018}).

\subsection{Control Flow and Execution Nodes}
The control flow nodes decide which actions and condition nodes to execute next. Three categories of control flow nodes (Sequence, Fallback and Parallel) exist. The two existing categories of execution nodes are actions and conditions, both are responsible for executing the actual behavior of the behavior tree. (\cite{collendanchise_behavior_2019}).


\begin{itemize}
	\item Control flow nodes
	\begin{itemize}
		\item \textbf{Sequence nodes} will execute its children nodes (routing the ticks) in sequence as long as each child returns "Success". If this has happened for every child, the sequence node itself will return "Success". In case one child returns "Failure" or "Running", the sequence node will immediately return the corresponding value and not route the tick to the next child.
		
		\item \textbf{Fallback nodes} executes its children one after another until one child returns "Success" or "Running", in which case it will itself returns "Success" or "Running" accordingly. As long as the childs status is "Failure", it will continue routing the ticks to the next child. If all children have returned "Failure", the fallback node will also return "Failure".
		
		\item \textbf{Parallel nodes} will execute all children simultaneously. Given a parameter m, it will decide on its return status. If more than m children succeeded, it will return "Success". If enough children have failed to make the "Success" impossible, "Failure" is returned, otherwise "Running".
	\end{itemize}
	\item Execution nodes
	\begin{itemize}
		\item \textbf{Action nodes} will execute a command after receiving a tick. They can return "Success", "Failure" as well as "Running". Action nodes typically execute the behavior of an agent, for example: "open door".
		
		\item \textbf{Condition nodes} will execute a command after receiving a tick. However contrary to the  Action Nodes, the can only return "Success" and "Failure". It is not possible for them execute over multiple ticks and thus can not return "Running". As suggested by their name, Condition Nodes typically check conditions like for example: "is door closed".
	\end{itemize}
\end{itemize}